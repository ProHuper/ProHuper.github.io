---
layout:     post
title:      "Spark学习笔记 01 环境配置"
subtitle:   "配置Spark时的一些坑点"
date:       2017-12-24 15:00:00
author:     "Huper"
tags:
    Spark
---

今天研究了一下午`Spark`的环境配置，不得不说这东西对`Windows`是真的不友好，一大堆问题，而且到现在还有几个问题没解决，强烈不建议在`win`上使用`Spark`！！！。现在将问题简单总结如下：

>- 解压路径里不要出现空格。
>- 解压Hadoop要用管理员权限。
>- Spark和Hadoop的版本要对应，这个会出现在Spark的文件名中。
>- 要自行去下载`winutils`，并放到`HADOOP_HOM`的`bin`目录下。
>- `spark-shell`使用的默认语言是Scala，要用python的话必须先进入`pySpark`。
>- 在`pySpark`中可以`import pyspark`，但是在`python` 的`REPL`中是不能直接导入的，可以直接将`\bin\python\pyspark`文件夹直接拷到python的扩展工具目录里。也可以使用`sys.path.insert`或者`sys.path.append`添加模块搜索路径。
>- java和Scala也可以直接在`maven`或者`sbt`中添加依赖使用`Spark`，这样会先从远程仓库下载依赖，比较方便。当然不想使用构建工具的话，可以将Spark和Hadoop解压包里的`jar`文件直接添加到项目依赖中。

还有两个问题一直没办法解决。在网上看了下是`windows`下使用`Spark`的通病：

>进入`spark-shell`的时候会出现警告：`Unable to load native-hadoop library for your platform...`。这个问题只找到了`Linux`下的解决方式，`windows`下完全不适用，本来想用`cygwin`改一下的，但是实在没心思了。
>
>退出`spark-shell`的时候会出现：` ERROR ShutdownHookManager: Exception while deleting Spark temp dir:`，没法删除临时文件，而且不是因为权限的原因。

不过好在这两个问题不影响使用（目前来说没什么影响）。不想看见的话可以在Spark的配置目录`conf`修改下`log4j`日志配置，将日志等级设置为`FATAL`，直接忽视就行了，但是你每次得手动清除临时文件，或者给弄成脚本。。。。

相比之下，`Linux`下的配置就简单多了，配置`Local`模式都不用装`Hadoop`，直接就可以愉快玩耍了。但是毕设还是要用`windows`做的，所以以后都是以`win`版为例。关于语言选择的问题，感觉推荐度是scala > java > python吧，这三个是`Spark`主要支持的语言。但是我个人感觉无所谓了，`Spark`更新到现在，对这三个语言的支持度已经差不多了，所以我之后都是用python为例学。当然如果你想从源码级别学习`Spark`，就果断选择scala吧。另外，做点小的项目你用`REPL`就搞定了，稍微大点的还是用`IDE`吧，推荐组合：

>java + intellij + maven/gradle
>
>scala + intellij + sbt
>
>python + pycharm (不需要构建工具了，直接使用pyspark)

好了就是这样，我们今天先测试一下配置好的`spark`，打开python或者`pyspark`：

```python
>>> import pyspark as ps
# 如果不是在pyspark的命令行中，一定要设置sc。
>>> sc = ps.SparkContext('local','pyspark')
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/12/25 14:49:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
>>> lines = sc.textFile('E:\io\data.txt')
>>> lines.count()
100
>>> print(lines.first())
3.542485        1.977398        -1
```

一个简单的文件行数统计测试，至于里面的sc是什么以后再讲吧。

